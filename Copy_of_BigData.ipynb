{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohitVempa51090/CrashSense-prediction/blob/master/Copy_of_BigData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Count Problem**\n",
        "1. Login to github\n",
        "2. Chose the code space of Jupyter notebook\n",
        "3. On the terminal comes up write ls command\n",
        "4. Go to the notebooks directory using the command cd notebooks\n",
        "5. Type in command nano word_count_data.txt – this will create a file and open a terminal.\n",
        "6. Type in a few words in the few lines.\n",
        "7. ^X to be use to exit. then Press Y as you want to save the same. Once you are\n",
        "back to the file on terminal you need to press enter to save the file and get\n",
        "back on the terminal.\n",
        "8. Now you can check the contents of the file using the cat word_count_data.txt\n",
        "command.\n",
        "9. Type nano mapper.py to create a mapper file again get on the editor window\n",
        "paste the mapper.py code.\n",
        "10.^X to be use to exit . then Press Y as you want to save the same. Once you are\n",
        "back to the file on terminal you need to press enter to save the file and get\n",
        "back on the terminal.\n",
        "11.Type nano reducer.py to create a reducer file to get to the editor window paste\n",
        "the reducer.py code.\n",
        "12.^X to be use to exit . then Press Y as you want to save the same. Once you are\n",
        "back to the file on terminal you need to press enter to save the file and get\n",
        "back on the terminal.\n",
        "13.Command cat word_count_data.txt | python3 mapper_sk.py – to see the mapper\n",
        "command output\n",
        "14.Command cat word_count_data.txt | python3 mapper_sk.py | sort -k1, 1 – to\n",
        "see the sorted output.\n",
        "15.Command cat word_count_data.txt | python3 mapper_sk.py | sort -k1, 1 |\n",
        "python reducer_sk.py – to see the final word count output.\n"
      ],
      "metadata": {
        "id": "oPL_bKPsJHEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE\n",
        "1. Mapper.py"
      ],
      "metadata": {
        "id": "F6AYGuBLJnZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "  words = line.split()\n",
        "\n",
        "  for word in words:\n",
        "    print('%s\\t%s' % (word, 1))"
      ],
      "metadata": {
        "id": "mke0xJWCJOYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. reducer.py\n"
      ],
      "metadata": {
        "id": "3Ajj48DHKkMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "   # remove leading and trailing whitespace\n",
        "   line = line.strip()\n",
        "   # splitting the data on the basis of tab we have provided in mapper.py\n",
        "   word, count = line.split('\\t', 1)\n",
        "   # convert count (currently a string) to int\n",
        "   try:\n",
        "       count = int(count)\n",
        "   except ValueError:\n",
        "       # count was not a number, so silently\n",
        "       # ignore/discard this line\n",
        "       continue\n",
        "\n",
        "   # this IF-switch only works because Hadoop sorts map output\n",
        "   # by key (here: word) before it is passed to the reducer\n",
        "   if current_word == word:\n",
        "       current_count += count\n",
        "   else:\n",
        "       if current_word:\n",
        "           # write result to STDOUT\n",
        "           print ('%s\\t%s' % (current_word, current_count))\n",
        "       current_count = count\n",
        "       current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "   print ('%s\\t%s' % (current_word, current_count))"
      ],
      "metadata": {
        "id": "5Ym_eLxAKoc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for PCY**"
      ],
      "metadata": {
        "id": "in_8rcEWKox4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "* The count_items function is used to count the occurrences of individual items in the given transaction data. It returns a dictionary (item_counts) where the keys are the items and the values are their respective counts.\n",
        "* The pcy function implements the PCY algorithm. It takes the transaction data,\n",
        "a support threshold, and the number of buckets for the hash table as input.\n",
        "* The item_counts dictionary is obtained by calling the count_items function on\n",
        "the transaction data.\n",
        "* The frequent_items set is initialized with the items whose counts meet or\n",
        "exceed the support threshold.\n",
        "* The frequent_itemsets set is initialized with the frequent_items set. This set will\n",
        "be updated iteratively to include frequent itemsets of increasing size.\n",
        "* The k variable is set to 2, representing the current size of the itemsets being\n",
        "generated.\n",
        "* The main loop starts, which continues until no more frequent itemsets of size k\n",
        "are found.\n",
        "* Inside the loop, candidate_itemsets is generated by taking combinations of\n",
        "items from the frequent_itemsets set with size k.\n",
        "* The itemset_counts dictionary is created to count the occurrences of each\n",
        "candidate itemset.\n",
        "* For each transaction, the code checks if a candidate itemset is a subset of that\n",
        "transaction using the issubset method. If it is, the count of the itemset is\n",
        "incremented.\n",
        "* The filtered_itemsets set is created by including only those itemsets whose\n",
        "count meets or exceeds the support threshold.\n",
        "* If no frequent itemsets of size k are found, the loop is terminated.\n",
        "* The frequent_itemsets set is updated by adding the filtered_itemsets to include\n",
        "the newly discovered frequent itemsets.\n",
        "* The value of k is incremented by 1 to move to the next itemset size.\n",
        "* Once the loop finishes, the function returns the frequent_itemsets set.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5PbbiEhrKtST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "def count_items(data):\n",
        "  item_counts = defaultdict(int)\n",
        "  for transaction in data:\n",
        "    for item in transaction:\n",
        "      item_counts[item] += 1\n",
        "  return item_counts\n",
        "\n",
        "def pcy(data, support, num_buckets):\n",
        "  item_counts = count_items(data)\n",
        "  frequent_items = {item for item, count in item_counts.items() if count >= support}\n",
        "  frequent_itemsets = frequent_items.copy()\n",
        "  k = 2\n",
        "  while True:\n",
        "    candidate_itemsets = set(combinations(frequent_itemsets, k))\n",
        "    itemset_counts = defaultdict(int)\n",
        "    for transaction in data:\n",
        "      for itemset in candidate_itemsets:\n",
        "        if set(itemset).issubset(transaction):\n",
        "          itemset_counts[itemset] += 1\n",
        "\n",
        "    filtered_itemsets = {itemset for itemset, count in itemset_counts.items() if count >= support}\n",
        "\n",
        "    if not filtered_itemsets:\n",
        "      break\n",
        "\n",
        "    frequent_itemsets.update(filtered_itemsets)\n",
        "    k += 1\n",
        "  return frequent_itemsets\n",
        "\n",
        "# Example usage\n",
        "data = [\n",
        "[1, 2, 3, 4],\n",
        "[1, 2, 4],\n",
        "[2, 3, 4],\n",
        "[2, 3],\n",
        "[1, 4],\n",
        "[1, 2, 3],\n",
        "[2, 3],\n",
        "[1, 3, 4]\n",
        "]\n",
        "\n",
        "frequent_itemsets = pcy(data, support=3, num_buckets=10)\n",
        "print(\"Frequent Itemsets:\")\n",
        "for itemset in frequent_itemsets:\n",
        "  print(itemset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FqII81iLGuk",
        "outputId": "924987e7-c355-4c1d-a788-95e584df11f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "(2, 4)\n",
            "(1, 2)\n",
            "(3, 4)\n",
            "(1, 4)\n",
            "(2, 3)\n",
            "(1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apriori**\n",
        "\n",
        "The Apriori algorithm is a classic algorithm for frequent itemset mining in data mining\n",
        "and association rule learning. It is used to identify sets of items that frequently occur\n",
        "together in a given dataset.\n",
        "The Apriori algorithm follows the following steps:\n",
        "1. Initialize:\n",
        " - Set a minimum support threshold.\n",
        " - Scan the dataset to count the support of each individual item and identify frequent\n",
        "items that meet the minimum support.\n",
        "2. Generate candidate itemsets of size 2:\n",
        " - Take the frequent items from the previous step and combine them to form candidate\n",
        "itemsets of size 2.\n",
        "3. Prune candidate itemsets:\n",
        " - Check if any subset of a candidate itemset of size 2 is infrequent (i.e., not in the list\n",
        "of frequent itemsets from the previous step). If any subset is infrequent, the candidate\n",
        "itemset is discarded.\n",
        "4. Count the support of candidate itemsets:\n",
        " - Scan the dataset to count the support of each candidate itemset of size 2.\n",
        "5. Generate frequent itemsets of size 2:\n",
        " - Select the candidate itemsets with support greater than or equal to the minimum\n",
        "support threshold. These itemsets become the frequent itemsets of size 2.\n",
        "6. Repeat steps 2 to 5 to generate candidate itemsets of size k and generate frequent\n",
        "itemsets of size k until no more frequent itemsets can be found.\n",
        "7. Terminate:\n",
        " - Stop the algorithm when no more frequent itemsets can be generated.\n",
        "Code:"
      ],
      "metadata": {
        "id": "lnNK3a0_M6M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "def count_support(itemsets, transactions):\n",
        "  support_counts = defaultdict(int)\n",
        "  for transaction in transactions:\n",
        "    for itemset in itemsets:\n",
        "      if set(itemset).issubset(transaction):\n",
        "        support_counts[itemset] += 1\n",
        "  return support_counts\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "  itemsets = [frozenset([item]) for transaction in transactions for item in transaction]\n",
        "  frequent_itemsets = []\n",
        "  k = 1\n",
        "  while itemsets:\n",
        "    support_counts = count_support(itemsets, transactions)\n",
        "    frequent_itemsets.extend(itemset for itemset, support in support_counts.items() if support >= min_support)\n",
        "    itemsets = set(combinations(set(item for itemset in itemsets for item in itemset), k + 1))\n",
        "    k += 1\n",
        "  return frequent_itemsets\n",
        "\n",
        "# Example usage\n",
        "transactions = [\n",
        "['A', 'B', 'C'],\n",
        "['A', 'C'],\n",
        "['A', 'B', 'D'],\n",
        "['B', 'D'],\n",
        "['A', 'C', 'D'],\n",
        "]\n",
        "\n",
        "min_support = 2\n",
        "\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "\n",
        "print(\"Frequent Itemsets:\")\n",
        "for itemset in frequent_itemsets:\n",
        "  print(itemset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kplQWwbCNOHv",
        "outputId": "b11a90e9-cc3d-40d4-a52d-0fad6a4e2005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "frozenset({'A'})\n",
            "frozenset({'B'})\n",
            "frozenset({'C'})\n",
            "frozenset({'D'})\n",
            "('C', 'A')\n",
            "('A', 'B')\n",
            "('A', 'D')\n",
            "('B', 'D')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bloom Filter**\n",
        "\n",
        "A Bloom filter is a probabilistic data structure used to test whether an element is a\n",
        "member of a set.The main\n",
        "purpose of a Bloom filter is to efficiently determine whether an element is present in a\n",
        "set or not, with the trade-off of allowing a small rate of false positives.The key characteristics of a Bloom filter are its space efficiency and constant-time\n",
        "membership test. It can represent a large set of elements using a relatively small\n",
        "amount of memory."
      ],
      "metadata": {
        "id": "VenqeoI9Q3iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybloom_live"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPC57iWzQQgi",
        "outputId": "a7b6340d-47fa-41ff-feb5-b90d41cd3db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybloom_live in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
            "Requirement already satisfied: bitarray>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pybloom_live) (2.7.5)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pybloom_live) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pybloom_live import BloomFilter\n",
        "# Create a Bloom filter with desired parameters\n",
        "bloom_filter = BloomFilter(capacity=1000, error_rate=0.01)\n",
        "# Add items to the Bloom filter\n",
        "bloom_filter.add(\"apple\")\n",
        "bloom_filter.add(\"banana\")\n",
        "bloom_filter.add(\"orange\")\n",
        "# Check if items are possibly in the Bloom filter\n",
        "print(\"apple\" in bloom_filter) # True\n",
        "print(\"banana\" in bloom_filter) # True\n",
        "print(\"orange\" in bloom_filter) # True\n",
        "print(\"grape\" in bloom_filter) # False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsdPXFd6RrY8",
        "outputId": "03a24c5f-eeb7-49e5-d328-41f86c0abb15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hadoop Commands**\n",
        "\n",
        "1. to start the Hadoop services:\n",
        "```\n",
        "sbin/start-all.sh\n",
        "```\n",
        "2. To check the Hadoop services are up and running\n",
        "```\n",
        "jps\n",
        "```\n",
        "3.  list all the files. Use lsr for recursive approach. It is useful when we want a hierarchy of a folder.\n",
        "```\n",
        "bin/hdfs dfs -ls  <path>\n",
        "```\n",
        "4. To create a directory. In Hadoop dfs there is no home directory by default. So let’s first create it.\n",
        "```\n",
        "bin/hdfs dfs -mkdir <folder name>\n",
        "```\n",
        "```\n",
        "creating home directory:\n",
        "```\n",
        "```\n",
        "hdfs/bin -mkdir /user\n",
        "hdfs/bin -mkdir /user/username -> write the username of your computer\n",
        "```\n",
        "5. touchz: It creates an empty file.\n",
        "```\n",
        "bin/hdfs dfs  -touchz  <file_path>\n",
        "```\n",
        "6. cat: To print file contents.\n",
        "```\n",
        "bin/hdfs dfs -cat <path>\n",
        "```\n",
        "7. rmr: This command deletes a file from HDFS recursively. It is very useful command when you want to delete a non-empty directory.\n",
        "```\n",
        "bin/hdfs dfs -rmr <filename/directoryName>\n",
        "```\n",
        "8. check out the list of dfs commands using the following command:\n",
        "```\n",
        "bin/hdfs dfs\n",
        "```\n"
      ],
      "metadata": {
        "id": "7KDWxDBWSkVL"
      }
    }
  ]
}